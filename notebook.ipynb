{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948a99f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.14.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (4.64.1)\n",
      "Requirement already satisfied: array-record in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.28.1)\n",
      "Requirement already satisfied: toml in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (3.20.3)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.14.1)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.26.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.4.0)\n",
      "Requirement already satisfied: promise in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: click in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (8.0.4)\n",
      "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.5.2)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.1.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.8.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2022.7.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from click->tensorflow-datasets) (0.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.61.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2587b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.15.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\lalyn\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e248e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "#import tensorflow_datasets as tdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492ea44a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4580\\2082476968.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import tensorflow_datasets as tfds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcassava_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cassava\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcassava_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcassava_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcassava_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcassava_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcassava_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "#import tensorflow_datasets as tfds\n",
    "cassava_data = tfds.load(\"cassava\")\n",
    "cassava_train, cassava_test = cassava_data[\"train\"], cassava_data[\"test\"]\n",
    "assert isinstance(cassava_train, tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc75256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_santa_dir = 'data/santa/'\n",
    "data_not_santa_dir = 'data/not_santa/'\n",
    "imgs_santa = [file for file in os.listdir(data_santa_dir) if file.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'cats_dogs_downsampled/train'\n",
    "validation_dir = 'cats_dogs_downsampled/val/'\n",
    "test_dir = 'cats_dogs_downsampled/test/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'data/train'\n",
    "test_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e102cc98",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cassava-disease.zip/train.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4580\\363920016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# loading the temp.zip and creating a zip object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/cassava-disease.zip/train.zip\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzObject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Extracting all the members of the zip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cassava-disease.zip/train.zip'"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "  \n",
    "# loading the temp.zip and creating a zip object\n",
    "with ZipFile(\"data/cassava-disease.zip/train.zip\", 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path='./')\n",
    "\n",
    "with ZipFile(\"data/cassava-disease.zip/test.zip\", 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path='./')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eab52e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cassava-disease.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"data\"))\n",
    "#print(os.listdir(\"../data/train/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b21f69a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid: './data/cassava-disease.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4580\\1512553642.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/cassava-disease.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: './data/cassava-disease.zip'"
     ]
    }
   ],
   "source": [
    "os.listdir('./data/cassava-disease.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c201c2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cassava-disease.zip',\n",
       " 'extraimages.zip',\n",
       " 'random.txt',\n",
       " 'sample_submission_file.csv',\n",
       " 'test.zip',\n",
       " 'train.zip']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcaab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "  \n",
    "# loading the temp.zip and creating a zip object\n",
    "with ZipFile(\"data/cassava-disease.zip\", 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24eaf59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train\\\\cbb\\\\train-cbb-0.jpg',\n",
       " './data/train\\\\cbb\\\\train-cbb-1.jpg',\n",
       " './data/train\\\\cbb\\\\train-cbb-10.jpg',\n",
       " './data/train\\\\cbb\\\\train-cbb-100.jpg',\n",
       " './data/train\\\\cbb\\\\train-cbb-101.jpg']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "Id = []\n",
    "for dirname, _, filenames in os.walk('./data/train'):\n",
    "    for filename in filenames:\n",
    "        Id.append(os.path.join(dirname, filename))\n",
    "Id[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d613d46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/train\\cbb\\train-cbb-0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/train\\cbb\\train-cbb-1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/train\\cbb\\train-cbb-10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/train\\cbb\\train-cbb-100.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/train\\cbb\\train-cbb-101.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             filename\n",
       "0    ./data/train\\cbb\\train-cbb-0.jpg\n",
       "1    ./data/train\\cbb\\train-cbb-1.jpg\n",
       "2   ./data/train\\cbb\\train-cbb-10.jpg\n",
       "3  ./data/train\\cbb\\train-cbb-100.jpg\n",
       "4  ./data/train\\cbb\\train-cbb-101.jpg"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.DataFrame()\n",
    "train=train.assign(filename=Id)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /kaggle/input/cassava-disease/extraimages.zip -d  /kaggle/working/ \n",
    "!unzip /kaggle/input/cassava-disease/test.zip -d /kaggle/working/\n",
    "!unzip /kaggle/input/cassava-disease/train.zip -d /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44de3dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cassava-disease.zip', 'extraimages.zip', 'random.txt', 'sample_submission_file.csv', 'test', 'test.zip', 'train', 'train.zip']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "print(os.listdir(\"./data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "646d8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(\"data/test.zip\", 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cca4e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(\"data/train.zip\", 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b09ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cassava-disease.zip', 'extraimages.zip', 'random.txt', 'sample_submission_file.csv', 'test', 'test.zip', 'train', 'train.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"./data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21d82683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cbb', 'cbsd', 'cgm', 'cmd', 'healthy']\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"./data/train\"))\n",
    "print(np.shape(os.listdir(\"./data/train\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46c62c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3774 images belonging to 1 classes.\n",
      "Found 5656 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = 'data/train'\n",
    "test_data_dir = 'data/test'\n",
    "\n",
    "# Get all the data in the directory data/validation (132 images), and reshape them\n",
    "test_generator = ImageDataGenerator().flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(64, 64), batch_size=3774)\n",
    "\n",
    "# Get all the data in the directory data/train (790 images), and reshape them\n",
    "train_generator = ImageDataGenerator().flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), batch_size=5656)\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86cc54f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67705ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58f56607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5656, 64, 64, 3)\n",
      "(5656, 5)\n",
      "(3774, 64, 64, 3)\n",
      "(3774, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_images))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(test_images))\n",
    "print(np.shape(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab5ad57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 5656)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_unrow = train_images.reshape(5656, -1).T\n",
    "\n",
    "# Preview the shape of train_img_unrow\n",
    "np.shape(train_img_unrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(5656, 784).astype('float32')\n",
    "X_test = X_test.reshape(3774, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e491618",
   "metadata": {},
   "source": [
    "## img class w MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(64, activation='tanh', input_shape=(784,)))\n",
    "model_1.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54668794",
   "metadata": {},
   "source": [
    "## Convolutional codealong- base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0912ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) # 2 hidden layers\n",
    "model.add(layers.Dense(7, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_img, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9abfaf",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=30,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(val_images, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705dff2",
   "metadata": {},
   "source": [
    "## CNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a17215",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
